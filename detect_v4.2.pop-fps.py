# import torch.nn.functional as F
# import torch
import numpy as np
# from LSTM.src.lstm import ActionClassificationLSTM as LSTM
# import pytorch_lightning as pl
# import pyopenpose as op
import cv2
import ntpath
import os
import sys

# sys.path.append('./openpose/build/python/openpose/Release')
# os.environ['PATH'] = os.environ['PATH'] + ';' + \
#     './openpose/build/x64/Release;' + './openpose/build/bin;'


video_path = '../Detected_JUST_TEST_IT_20230317_v28_puttext_null.mp4'
cap = cv2.VideoCapture(video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
# fps = int(cap.get(cv2.CAP_PROP_FPS))
fps = 15
tot_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
file_name = ntpath.basename(video_path)
vid_writer = cv2.VideoWriter('Detected_{}'.format(
    file_name), fourcc, 30, (width, height))

# counter = 0
# buffer_window = []
# label = None
# SKIP_FRAME_COUNT = 0

# # OpenPose Î™®Îç∏ÏùÑ Î°úÎìú
# # Custom Params (refer to include/openpose/flags.hpp for more parameters)
# params = dict()
# params["model_folder"] = "./openpose/models/"

# # Starting OpenPose
# opWrapper = op.WrapperPython()
# opWrapper.configure(params)
# opWrapper.start()

# # Process Image
# datum = op.Datum()


# # LSTM Î™®Îç∏ Î°úÎìú
# CKPT_PATH = 'LSTM/lightning_logs/version_28/checkpoints/epoch=94-step=1139.ckpt'
# lstm_classifier = LSTM.load_from_checkpoint(CKPT_PATH)
# print(lstm_classifier)
# lstm_classifier.eval()


# LABELS = {
#     0: "Object_Manipulation",
#     1: "Wrench_job",
# }


# idx = 0
# buffer_window = []


sec = 0
frameRate = 0.066  # //it will capture image in each 0.5 second

while True:

    sec = sec + frameRate
    sec = round(sec, 2)

    cap.set(cv2.CAP_PROP_POS_MSEC, sec*1000)

    ret, frame = cap.read()
    if ret == False:
        break
    img = frame.copy()

    # datum.cvInputData = img
    # opWrapper.emplaceAndPop(op.VectorDatum([datum]))
    # img = datum.cvOutputData
    # print(f'{idx}/{tot_frames}')
    # idx += 1
    # try:  # üéÉ Ïã†Í∑ú Ï∂îÍ∞Ä, Ïû•Î©¥Ïóê ÏÇ¨ÎûåÏù¥ nullÏùº Í≤ΩÏö∞Î•º Î∞òÏòÅ
    #     features = []
    #     human_1 = datum.poseKeypoints[0]

    #     for i, row in enumerate(human_1):
    #         features.append(row[0])
    #         features.append(row[1])

    #     # print(features) # featuresÏóê 50Í∞úÏùò x,y Ï¢åÌëúÍ∞Ä listÎ°ú Îã¥Í∏∞Í≤å ÎêúÎã§.

    #     if len(buffer_window) < 4:  # -----------  32->8->4->5Î°ú Î≥ÄÍ≤Ω ----------- #
    #         buffer_window.append(features)

    #     else:
    #         model_input = torch.Tensor(
    #             np.array(buffer_window, dtype=np.float32))
    #         model_input = torch.unsqueeze(model_input, dim=0)
    #         y_pred = lstm_classifier(model_input)
    #         prob = F.softmax(y_pred, dim=1)
    #         pred_index = prob.data.max(dim=1)[1]
    #         buffer_window.pop(0)
    #         buffer_window.pop(0)
    #         buffer_window.pop(0)
    #         buffer_window.pop(0)
    #         buffer_window.append(features)
    #         label = LABELS[pred_index.numpy()[0]]
    #         print('label:', pred_index.numpy()[0])
    # except:
    #     pass

    # if label is not None:
    #     cv2.putText(img, f'Action:{label}',
    #                 (int(width-400), height-50), cv2.FONT_HERSHEY_COMPLEX, 0.9, (102, 255, 255), 2)
    #     label = None

    # else:
    #     cv2.putText(img, 'Null',
    #                 (int(width-400), height-50), cv2.FONT_HERSHEY_COMPLEX, 0.9, (102, 255, 255), 2)

    vid_writer.write(img)

cap.release()
vid_writer.release()
